version: '3.8'

x-vllm-base: &vllm-base
  image: vllm/vllm-openai:v0.6.3.post1
  volumes:
    - /mnt/models:/models:ro
    - vllm-cache:/root/.cache/huggingface
  environment:
    - HF_TOKEN=${HF_TOKEN:?Error: HF_TOKEN environment variable is required}
    - CUDA_VISIBLE_DEVICES=0
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  restart: "no"  # Important: orchestrator manages lifecycle
  network_mode: bridge

services:
  model-orchestrator:
    build: ./services/model-orchestrator
    container_name: multimodal-stack-orchestrator
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - DOCKER_HOST=unix:///var/run/docker.sock
    ports:
      - "8888:8888"
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 10s
      timeout: 5s
      retries: 3

  mistral-llm:
    <<: *vllm-base
    container_name: multimodal-stack-mistral-llm-1
    command:
      - --model=/models/mistral-awq
      - --served-model-name=mistral
      - --max-model-len=8192
      - --gpu-memory-utilization=0.85
      - --host=0.0.0.0
      - --port=8000
    ports:
      - "8000:8000"

  llava-llm:
    <<: *vllm-base
    container_name: multimodal-stack-llava-llm-1
    command:
      - --model=/models/llava-7b
      - --tokenizer=liuhaotian/llava-v1.5-7b
      - --served-model-name=llava
      - --trust-remote-code
      - --download-dir=/models
      - --tokenizer-mode=auto
      - --max-model-len=4096
      - --gpu-memory-utilization=0.10
      - --disable-custom-all-reduce
      - --image-input-type=pixel_values
      - --host=0.0.0.0
      - --port=8000
    ports:
      - "8000:8000"

  coder-llm:
    <<: *vllm-base
    container_name: multimodal-stack-coder-llm-1
    command:
      - --model=/models/deepseek-coder-6.7b-gptq
      - --served-model-name=coder,deepseek
      - --quantization=gptq
      - --max-model-len=4096
      - --gpu-memory-utilization=0.85
      - --host=0.0.0.0
      - --port=8000
    ports:
      - "8000:8000"

  api-gateway:
    build: ./services/api-gateway
    container_name: multimodal-stack-gateway
    ports:
      - "8080:8080"
    environment:
      - ORCHESTRATOR_URL=http://model-orchestrator:8888
      - LLM_URL=http://host.docker.internal:8000
    restart: unless-stopped
    depends_on:
      - model-orchestrator
    extra_hosts:
      - "host.docker.internal:host-gateway"

volumes:
  vllm-cache:

networks:
  default:
    name: multimodal-stack
