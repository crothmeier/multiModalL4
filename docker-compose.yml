x-vllm-base: &vllm-base
  runtime: nvidia
  environment:
    NVIDIA_VISIBLE_DEVICES: all
    NVIDIA_DRIVER_CAPABILITIES: compute,utility
    HF_TOKEN: ${HF_TOKEN:-}
  volumes:
    - /mnt/models:/models:ro
  deploy:
    resources:
      reservations:
        devices:
          - capabilities: [gpu]
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  cap_add:
    - NET_BIND_SERVICE

services:
  general-llm:
    <<: *vllm-base
    build: ./services/vllm-general
    image: multimodal-stack/vllm-general:latest
    command:
      - --served-model-name=mistral
      - --model=/models/mistral-awq
      - --tokenizer=mistralai/Mistral-7B-Instruct-v0.2
      - --trust-remote-code
      - --download-dir=/tmp
      - --max-model-len=8192
      - --gpu-memory-utilization=0.50
      - --host=0.0.0.0
      - --port=8000

  coder-llm:
    <<: *vllm-base
    build: ./services/vllm-specialized
    image: multimodal-stack/vllm-specialized:latest
    ports:
      - "8001:8001"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    command:
      # DeepSeek Coder 6.7B GPTQ
      - --model=/models/deepseek-gptq
      - --served-model-name=coder
      - --quantization=gptq
      - --dtype=half
      - --trust-remote-code
      - --max-model-len=4096
      - --gpu-memory-utilization=0.45
      - --enable-chunked-prefill
      - --enable-prefix-caching
      - --max-num-batched-tokens=2048
      - --host=0.0.0.0
      - --port=8001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  llava-llm:
    <<: *vllm-base
    build: ./services/vllm-llava
    image: multimodal-stack/vllm-llava:latest
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8002:8000"
    command:
      - --served-model-name=llava
      - --model=llava-hf/llava-1.5-7b-hf
      - --trust-remote-code
      - --tokenizer-mode=auto
      - --dtype=half
      - --max-model-len=4096
      - --gpu-memory-utilization=0.90
      - --download-dir=/tmp
      - --host=0.0.0.0
      - --port=8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3

  api:
    build: ./services/api-gateway
    image: multimodal-stack/api-gateway:latest
    ports:
      - "8080:8080"
    environment:
      ROUTES: |
        /chat/completions -> general-llm:8000/v1
        /code/completions -> specialized-llm:8001/v1
        /vision/completions -> specialized-llm:8001/v1
    depends_on:
      - general-llm
      - coder-llm
      - llava-llm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 3s
      retries: 3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE

  # Whisper & Piper remain in 'full' profile - see README for usage
  whisper:
    image: ghcr.io/ggerganov/whisper.cpp:v1.6.1-cuda12
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    volumes:
      - /tmp/audio:/audio:ro
    profiles:
      - full

  piper:
    image: rhasspy/piper:1.2.0
    volumes:
      - ./models/piper:/home/piper/.local/share/piper/voices:ro
    profiles:
      - full

  # Prometheus disabled until config exists (Phase 2)
  # prometheus:
  #   image: prom/prometheus:v2.52.0
  #   volumes:
  #     - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #   ports:
  #     - "9090:9090"
  #   profiles:
  #     - monitoring

  dcgm:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.5-3.4.1-ubuntu22.04
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    ports:
      - "9400:9400"
    profiles:
      - monitoring
