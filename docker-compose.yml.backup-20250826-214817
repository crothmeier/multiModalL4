version: "3.8"
x-vllm-base: &vllm-base
  image: vllm/vllm-openai:v0.6.3
  volumes:
    - /mnt/models:/models:ro
    - ./logs:/logs
  environment:
    - HF_TOKEN=${HF_TOKEN:?Error: HF_TOKEN environment variable is required}
    - VLLM_API_KEY=${VLLM_API_KEY:-default-key}
    - CUDA_VISIBLE_DEVICES=0
    - VLLM_WORKER_MULTIPROC_METHOD=spawn
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  restart: unless-stopped
  ipc: host
  shm_size: 16gb
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
services:
  mistral:
    <<: *vllm-base
    container_name: vllm-mistral
    ports:
      - "8001:8000"
    command: >
      --model /models/mistral-7b-awq/
      --quantization awq
      --dtype auto
      --api-key ${VLLM_API_KEY:-default-key}
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.30
      --max-num-seqs 8
      --enable-prefix-caching
      --disable-log-requests
      --trust-remote-code
    profiles:
      - mistral

  deepseek-coder:
    <<: *vllm-base
    container_name: vllm-deepseek-coder
    ports:
      - "8002:8000"
    command: >
      --model /models/deepseek-coder-gptq/
      --quantization gptq
      --dtype auto
      --api-key ${VLLM_API_KEY:-default-key}
      --host 0.0.0.0
      --port 8000
      --max-model-len 8192
      --gpu-memory-utilization 0.45
      --max-num-seqs 4
      --enable-prefix-caching
      --disable-log-requests
      --trust-remote-code
    profiles:
      - coder

  llava:
    <<: *vllm-base
    container_name: vllm-llava
    ports:
      - "8003:8000"
    command: >
      --model /models/llava/
      --dtype auto
      --api-key ${VLLM_API_KEY:-default-key}
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.80
      --max-num-seqs 2
      --enable-prefix-caching
      --disable-log-requests
      --trust-remote-code
      --chat-template template_llava.jinja
      --image-input-type pixel_values
      --image-token-id 32000
      --image-input-shape 1,3,336,336
      --image-feature-size 576
    profiles:
      - llava

  model-manager:
    image: alpine:latest
    container_name: model-manager
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./scripts:/scripts:ro
    command: >
      sh -c "
      apk add --no-cache docker-cli curl jq &&
      echo 'Model Manager ready. Use docker compose profiles to switch models:' &&
      echo '  docker compose --profile mistral up -d' &&
      echo '  docker compose --profile coder up -d' &&
      echo '  docker compose --profile llava up -d' &&
      echo 'Stop all: docker compose down' &&
      tail -f /dev/null
      "
    profiles:
      - manager
networks:
  default:
    name: multimodal-llm-network
    driver: bridge
